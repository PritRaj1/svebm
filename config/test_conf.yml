model:
  class_path: src.model.SVEBM

  init_args:
    data_dim: null
    latent_dim: 128
    num_classes: 10
    memory_dim: 128
    
    ebm_model:
      class_path: src.ebm.ebm_model.EBM_fcn
      init_args:
        latent_dim: ${model.init_args.latent_dim}
        num_classes: ${model.init_args.num_classes}
        hidden_layers: [256, 128]
        activation: relu
        num_latent_samples: 20
        num_gmm_components: 5
        eta: 1.0
        N: 1

    encoder_model:
      class_path: src.variational.encoder_model.EncoderModel
      init_args:
        input_dim: ${model.init_args.data_dim}
        memory_dim: ${model.init_args.memory_dim}
        latent_dim: ${model.init_args.latent_dim}
        hidden_layers: [256, 128]
        nhead: 8
        dropout: 0.1
        activation: relu
        pad_id: 0

    decoder_model:
      class_path: src.variational.decoder_model.DecoderModel
      init_args:
        vocab_size: 30522
        embed_size: 768
        latent_dim: ${model.init_args.latent_dim}
        memory_dim: ${model.init_args.memory_dim}
        hidden_layers: [128, 256]
        nhead: 8
        dropout: 0.1
        activation: relu
        max_dec_len: 50
        pad_id: 0
        bos_id: 1
        eos_id: 2
        unk_id: 3
        concat_latent: false

    logprob:
      class_path: src.criterion.LogProb
      init_args:
        ignore_index: 0
        cls_id: 0
        kl_weight: 1.0
        nll_weight: 1.0

    learning_rate: 0.001

data:
  class_path: src.data.TextDataModule

  init_args:
    dataset_cls: datasets.load_dataset
    dataset_kwargs:
      path: imdb
      split: train
      
    batch_size: 32
    num_workers: 4
    val_split: 0.1
    test_split: 0.1
    shuffle: true
    pin_memory: true
    drop_last: false
    seed: 42
    auto_detect_dim: true
    collate_fn: null

trainer:
  max_epochs: 100
  accelerator: auto
  devices: auto
  precision: 16-mixed 