model:
  class_path: src.model.SVEBM

  init_args:
    data_dim: 768
    latent_dim: 128
    
    ebm_model:
      class_path: src.ebm.ebm_model.EBM_fcn
      init_args:
        latent_dim: 128
        num_classes: 10
        hidden_layers: [256, 128]
        activation: relu
        num_latent_samples: 20
        num_gmm_components: 5
        eta: 1.0
        N: 1

    encoder_model:
      class_path: src.variational.encoder_model.EncoderModel
      init_args:
        input_dim: 768
        memory_dim: 128
        latent_dim: 128
        hidden_layers: [256, 128]
        nhead: 8
        dropout: 0.1
        activation: relu
        pad_id: 0

    decoder_model:
      class_path: src.variational.decoder_model.DecoderModel
      init_args:
        vocab_size: 30522
        embed_size: 768
        latent_dim: 128
        memory_dim: 128
        hidden_layers: [128, 256]
        nhead: 8
        dropout: 0.1
        activation: relu
        max_dec_len: 50
        pad_id: 0
        bos_id: 1
        eos_id: 2
        unk_id: 3
        concat_latent: false

    loss_struct:
      class_path: src.criterion.LogProb
      init_args:
        ignore_index: 0
        cls_id: 0
        kl_weight: 1.0
        nll_weight: 1.0

    learning_rate: 0.001
    ebm_learning_rate: 0.0001
    gen_type: greedy

    kl_annealer:
      class_path: src.variational.kl_annealing.KLAnnealer
      init_args:
        total_steps: 10000
        n_cycle: 4
        ratio_increase: 0.25
        ratio_zero: 0.5
        max_kl_weight: 0.5

data:
  class_path: src.data.TextDataModule

  init_args:
    dataset_cls: datasets.load_dataset
    dataset_kwargs:
      path: imdb
      split: train
      
    batch_size: 32
    num_workers: 4
    val_split: 0.1
    test_split: 0.1
    shuffle: true
    pin_memory: true
    drop_last: false
    seed: 42
    auto_detect_dim: true
    collate_fn:
      class_path: src.collate.create_text_collator
      init_args:
        tokenizer_name: bert-base-uncased
        max_length: 128
        embed_dim: 768
        num_latent_samples: 20
        num_gmm_components: 5

trainer:
  max_epochs: 100
  accelerator: auto
  devices: auto
  precision: 16-mixed 